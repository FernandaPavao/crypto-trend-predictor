# pessoa2_ml/treino.py
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import pickle
import os

# Features que ser√£o usadas no modelo
FEATURE_COLUMNS = [
    'price_usd', 'preco_variacao_1h', 'preco_variacao_6h', 
    'preco_variacao_12h', 'preco_variacao_24h',
    'media_movel_6h', 'media_movel_12h', 'media_movel_24h',
    'volatilidade_6h', 'volatilidade_24h',
    'max_24h', 'min_24h', 'rsi'
]

def treinar_modelo(df_features):
    """
    Treina Random Forest Classifier com valida√ß√£o cruzada.
    
    Args:
        df_features: DataFrame com features e target
        
    Returns:
        tuple: (modelo, scaler, feature_columns)
    """
    print("\n" + "="*60)
    print("ü§ñ TREINANDO MODELO DE MACHINE LEARNING")
    print("="*60)
    
    # Separar features e target
    X = df_features[FEATURE_COLUMNS]
    y = df_features['target']
    
    print(f"\nüìä Dataset:")
    print(f"   Features: {len(FEATURE_COLUMNS)}")
    print(f"   Amostras: {len(X)}")
    print(f"   Classes: {y.nunique()}")
    
    # Split treino/teste com estratifica√ß√£o
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nüìà Divis√£o dos dados:")
    print(f"   Treino: {len(X_train)} amostras ({len(X_train)/len(X)*100:.1f}%)")
    print(f"   Teste: {len(X_test)} amostras ({len(X_test)/len(X)*100:.1f}%)")
    
    # Normaliza√ß√£o dos dados
    print("\nüîÑ Normalizando dados com StandardScaler...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Criar e treinar modelo
    print("\nüå≤ Treinando Random Forest...")
    modelo = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    
    modelo.fit(X_train_scaled, y_train)
    print("‚úÖ Modelo treinado!")
    
    # Fazer previs√µes
    y_pred = modelo.predict(X_test_scaled)
    
    # AVALIA√á√ÉO DO MODELO
    print("\n" + "="*60)
    print("üìä RESULTADOS DO MODELO")
    print("="*60)
    
    # Acur√°cia
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n‚úÖ Acur√°cia no teste: {accuracy:.2%}")
    
    # Relat√≥rio de classifica√ß√£o
    print("\nüìã Relat√≥rio Detalhado:")
    print(classification_report(y_test, y_pred, 
                                target_names=['‚¨áÔ∏è  QUEDA (0)', '‚¨ÜÔ∏è  SUBIDA (1)']))
    
    # Valida√ß√£o cruzada
    print("\nüîÑ Valida√ß√£o Cruzada (5-fold):")
    cv_scores = cross_val_score(modelo, X_train_scaled, y_train, cv=5, scoring='accuracy')
    print(f"   Scores: {[f'{s:.2%}' for s in cv_scores]}")
    print(f"   M√©dia: {cv_scores.mean():.2%} (¬±{cv_scores.std():.2%})")
    
    # Matriz de Confus√£o
    print("\nüìä Gerando gr√°ficos de avalia√ß√£o...")
    os.makedirs('graficos', exist_ok=True)
    
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',
                xticklabels=['QUEDA', 'SUBIDA'],
                yticklabels=['QUEDA', 'SUBIDA'],
                cbar_kws={'label': 'Quantidade'})
    plt.title('Matriz de Confus√£o', fontsize=14, fontweight='bold')
    plt.ylabel('Real', fontsize=12)
    plt.xlabel('Previsto', fontsize=12)
    plt.tight_layout()
    plt.savefig('graficos/matriz_confusao.png', dpi=300, bbox_inches='tight')
    print("   üíæ Matriz de confus√£o: graficos/matriz_confusao.png")
    plt.close()
    
    # Import√¢ncia das Features
    importance_df = pd.DataFrame({
        'feature': FEATURE_COLUMNS,
        'importance': modelo.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüîù Top 5 Features mais importantes:")
    for idx, row in importance_df.head().iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")
    
    plt.figure(figsize=(10, 6))
    sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')
    plt.title('Import√¢ncia das Features', fontsize=14, fontweight='bold')
    plt.xlabel('Import√¢ncia', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.tight_layout()
    plt.savefig('graficos/feature_importance.png', dpi=300, bbox_inches='tight')
    print("   üíæ Feature importance: graficos/feature_importance.png")
    plt.close()
    
    print("="*60)
    
    return modelo, scaler, FEATURE_COLUMNS


def salvar_modelo(modelo, scaler, feature_columns):
    """
    Salva modelo, scaler e feature_columns em arquivos pickle.
    
    Args:
        modelo: Modelo treinado
        scaler: StandardScaler ajustado
        feature_columns: Lista de colunas usadas
    """
    print("\nüíæ Salvando modelo e artefatos...")
    
    # Criar pasta models
    os.makedirs("models", exist_ok=True)
    
    # Salvar modelo
    with open("models/modelo_crypto_classifier.pkl", "wb") as f:
        pickle.dump(modelo, f)
    print("   ‚úÖ Modelo: models/modelo_crypto_classifier.pkl")
    
    # Salvar scaler
    with open("models/scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)
    print("   ‚úÖ Scaler: models/scaler.pkl")
    
    # Salvar feature columns
    with open("models/feature_columns.pkl", "wb") as f:
        pickle.dump(feature_columns, f)
    print("   ‚úÖ Features: models/feature_columns.pkl")
    
    print("\n‚úÖ Todos os artefatos salvos com sucesso!")